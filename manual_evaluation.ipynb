{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "warnings.filterwarnings('ignore', category=InsecureRequestWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "project_id = os.getenv('VERTEXAI_PROJECT_ID')\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = GenerativeModel(\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    response = requests.get(image_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    return response.content\n",
    "\n",
    "def load_image_from_url(image_url: str) -> PIL_Image.Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Gemini/llm_responses_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Title', 'Body', 'Title_Body', 'ImageURLs', 'llm_zero_shot_title',\n",
       "       'llm_zero_shot_body', 'llm_zero_shot_combined', 'llm_few_shot_title',\n",
       "       'llm_few_shot_body', 'llm_few_shot_combined', 'llm_cot_title',\n",
       "       'llm_cot_body', 'llm_cot_combined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Stack Overflow post ID: 79124400\n",
      "================================================================================\n",
      "\n",
      "Processing Gemini model with zero_shot prompting...\n",
      "\n",
      "Gemini zero_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8/10\n",
      "Reasoning: The image shows the VS Code Source Control tab with the \"Commit\" button available. The Stack Overflow question is about customizing the behavior of this button to always include the \"and Push\" option. The image directly illustrates the area of the VS Code interface the question is about.\n",
      "\n",
      "LLM-Image Rating: 9/10\n",
      "Reasoning: The Gemini response focuses on a scenario where the VS Code Source Control tab is empty and the user cannot commit changes. The image clearly shows an empty Source Control tab, making the generated question highly relevant to the image.\n",
      "\n",
      "LLM-SO Rating: 2/10\n",
      "Reasoning: The Gemini response and the original Stack Overflow question both deal with the VS Code Source Control tab and committing changes. However, the specific issues and desired outcomes are different. The Stack Overflow question is about customization, while the Gemini response focuses on troubleshooting an empty Source Control tab. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing Gemini model with few_shot prompting...\n",
      "\n",
      "Gemini few_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8\n",
      "Reasoning: The image shows the VS Code Source Control panel with the \"Commit\" button visible. The Stack Overflow question is specifically about changing the behavior of this button to always show \"Commit and Push\". The image directly relates to the core issue presented in the question.\n",
      "\n",
      "LLM-Image Rating: 9\n",
      "Reasoning: The image clearly shows two source control providers with the same name (\"unique\") in VS Code. The LLM-generated question addresses this exact scenario, asking how to differentiate between two providers with identical names in the Source Control view.\n",
      "\n",
      "LLM-SO Rating: 2\n",
      "Reasoning: The LLM-generated question focuses on differentiating between duplicate source control providers, while the original Stack Overflow question is about changing the default behavior of the \"Commit\" button. These are entirely different issues within the context of VS Code's Source Control functionality. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing Gemini model with cot prompting...\n",
      "\n",
      "Gemini cot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8/10\n",
      "Reasoning: The Stack Overflow question directly asks about the \"Commit and Push\" button in VS Code, which is not visible in the image. However, the image clearly shows the VS Code interface with the \"Source Control\" tab open and a \"Commit\" button present. The question's context heavily implies this interface, making the image highly relevant.\n",
      "\n",
      "LLM-Image Rating: 9/10\n",
      "Reasoning: The Gemini generated question focuses on entering a commit message in VS Code, specifically mentioning the \"Message (#Enter to commit on \"main\")\" placeholder. This placeholder is directly visible in the image, creating a strong visual connection between the LLM's question and the provided screenshot.\n",
      "\n",
      "LLM-SO Rating: 3/10\n",
      "Reasoning: While both the original Stack Overflow question and the LLM-generated question revolve around the VS Code Git interface, they address different aspects. The original question seeks to customize button behavior, while the LLM focuses on a perceived issue with entering a commit message. The core functionalities differ, resulting in a low similarity score. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing GPT-4o model with zero_shot prompting...\n",
      "\n",
      "GPT-4o zero_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8\n",
      "Reasoning: The image shows the VS Code Source Control panel with the \"Commit\" button visible. The question is about changing the default behavior of this button to always \"Commit and Push\". The image directly relates to the core element of the question.\n",
      "\n",
      "LLM-Image Rating: 2\n",
      "Reasoning: The image shows a functional \"Commit\" button, while the LLM-generated question focuses on a scenario where the \"Commit\" button is disabled. There's a disconnect between the image's depiction of a working commit feature and the LLM's focus on a disabled commit feature.\n",
      "\n",
      "LLM-SO Rating: 3\n",
      "Reasoning: Both the original question and the LLM-generated question revolve around the \"Commit\" functionality in VS Code's Source Control. However, the original question seeks to modify the default behavior, while the LLM focuses on troubleshooting a disabled \"Commit\" button. The topics are related but address different aspects of the same feature. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing GPT-4o model with few_shot prompting...\n",
      "\n",
      "GPT-4o few_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8\n",
      "Reasoning: The image shows the VS Code Source Control panel with the \"Commit\" button visible. The Stack Overflow question is specifically about customizing the behavior of this button (\"Commit\" vs \"Commit and Push\"). The image directly relates to the core issue raised in the question.\n",
      "\n",
      "LLM-Image Rating: 9\n",
      "Reasoning: The image depicts the VS Code Source Control panel, which is directly related to the LLM-generated question about using Source Control in VS Code. The question's focus on committing, branches, and best practices aligns well with the functionalities typically accessed through this interface.\n",
      "\n",
      "LLM-SO Rating: 3\n",
      "Reasoning: While both the original Stack Overflow question and the LLM-generated question touch upon source control in VS Code, their specific concerns differ significantly. The original question seeks a solution for a very specific UI customization, while the LLM-generated question is broader, asking about general source control best practices. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing GPT-4o model with cot prompting...\n",
      "\n",
      "GPT-4o cot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 8/10\n",
      "Reasoning: The image shows the VS Code Source Control panel with the \"Commit\" button visible. The Stack Overflow question is about customizing the behavior of this button (always \"Commit and Push\"). The image clearly illustrates the element in question, even if it doesn't show the dropdown menu the user mentioned.\n",
      "\n",
      "LLM-Image Rating: 7/10\n",
      "Reasoning: The image shows an active \"Commit\" button, while the LLM-generated question focuses on a scenario where the \"Commit\" button is disabled. However, the overall context of the image (Source Control panel in VS Code) aligns with the LLM's question about commit issues.\n",
      "\n",
      "LLM-SO Rating: 3/10\n",
      "Reasoning: The LLM-generated question addresses a different problem (disabled \"Commit\" button) than the original Stack Overflow post (customizing the button's behavior to always \"Commit and Push\"). While both relate to the \"Commit\" button in VS Code's Source Control, the specific issues and desired outcomes are distinct. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing llama-3.2 model with zero_shot prompting...\n",
      "\n",
      "llama-3.2 zero_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 10\n",
      "Reasoning: The image shows the VS Code Source Control panel with the \"Commit\" button visible. The Stack Overflow question is specifically asking about how to change the behavior of this button to always show \"Commit and Push\". The question directly relates to the content shown in the image.\n",
      "\n",
      "LLM-Image Rating: 1\n",
      "Reasoning: The LLM response discusses a UI rendering issue with React and Material-UI, providing code snippets and expected behavior. The image is a screenshot of VS Code's Source Control panel and has no relation to React, Material-UI, or UI rendering problems.\n",
      "\n",
      "LLM-SO Rating: 1\n",
      "Reasoning: The LLM response and the original Stack Overflow question are completely unrelated. The Stack Overflow question is about a specific feature in VS Code's Source Control, while the LLM response discusses a UI rendering problem in a React application. There is no overlap in the topics or content. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing llama-3.2 model with few_shot prompting...\n",
      "\n",
      "llama-3.2 few_shot Analysis:\n",
      "----------------------------------------\n",
      "ANALYSIS:\n",
      "Image-Question Rating: 10\n",
      "Reasoning: The image perfectly illustrates the Stack Overflow question. It shows the VS Code interface with the \"Commit\" button that the user wants to change to \"Commit and Push.\"\n",
      "\n",
      "LLM-Image Rating: 1\n",
      "Reasoning: The LLM-generated response has no relationship to the image. It discusses a Python memory error and doesn't mention anything related to VS Code, commits, or dropdowns.\n",
      "\n",
      "LLM-SO Rating: 1\n",
      "Reasoning: The LLM-generated response is completely unrelated to the original Stack Overflow question. The original question asks about changing a button in VS Code, while the LLM response talks about a Python memory error. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Waiting 10 seconds before next analysis...\n",
      "\n",
      "Processing llama-3.2 model with cot prompting...\n",
      "\n",
      "llama-3.2 cot Analysis:\n",
      "----------------------------------------\n",
      "## ANALYSIS:\n",
      "\n",
      "**Image-Question Rating: 10/10**\n",
      "**Reasoning:** The image perfectly illustrates the Stack Overflow question. It shows the VS Code Source Control panel with the \"Commit\" button that the user wants to change to \"Commit and Push.\" \n",
      "\n",
      "**LLM-Image Rating: 1/10**\n",
      "**Reasoning:** The LLM-generated response bears no relation to the image. It discusses a \"Cannot resolve symbol\" error in Eclipse, which is not depicted or hinted at in the provided VS Code screenshot.\n",
      "\n",
      "**LLM-SO Rating: 1/10**\n",
      "**Reasoning:** The LLM response is completely unrelated to the original Stack Overflow question. The original question asks about changing the default commit behavior in VS Code, while the LLM response talks about resolving a compilation error in Eclipse. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Analysis Summary:\n",
      "================================================================================\n",
      "\n",
      "Average Ratings by Model:\n",
      "           image_question_rating  llm_image_rating  llm_so_rating\n",
      "model                                                            \n",
      "GPT-4o                       8.0               6.0       3.000000\n",
      "Gemini                       8.0               9.0       2.333333\n",
      "llama-3.2                   10.0               1.0       1.000000\n",
      "\n",
      "Average Ratings by Prompting Strategy:\n",
      "           image_question_rating  llm_image_rating  llm_so_rating\n",
      "strategy                                                         \n",
      "cot                     8.666667          5.666667       2.333333\n",
      "few_shot                8.666667          6.333333       2.000000\n",
      "zero_shot               8.666667          4.000000       2.000000\n",
      "\n",
      "Detailed Ratings by Model and Strategy:\n",
      "                     image_question_rating  llm_image_rating  llm_so_rating\n",
      "model     strategy                                                         \n",
      "GPT-4o    cot                          8.0               7.0            3.0\n",
      "          few_shot                     8.0               9.0            3.0\n",
      "          zero_shot                    8.0               2.0            3.0\n",
      "Gemini    cot                          8.0               9.0            3.0\n",
      "          few_shot                     8.0               9.0            2.0\n",
      "          zero_shot                    8.0               9.0            2.0\n",
      "llama-3.2 cot                         10.0               1.0            1.0\n",
      "          few_shot                    10.0               1.0            1.0\n",
      "          zero_shot                   10.0               1.0            1.0\n",
      "\n",
      "Detailed analysis saved to: Data/analysis_id_79124400_detailed.csv\n",
      "\n",
      "Analysis complete. Thank you!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from dotenv import load_dotenv\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerationConfig, GenerativeModel, Image, Part\n",
    "from PIL import Image as PIL_Image\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore', category=InsecureRequestWarning)\n",
    "\n",
    "def init_vertexai():\n",
    "    \"\"\"Initialize Vertex AI with credentials\"\"\"\n",
    "    load_dotenv()\n",
    "    project_id = os.getenv('VERTEXAI_PROJECT_ID')\n",
    "    vertexai.init(project=project_id, location=\"us-central1\")\n",
    "    return GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    \"\"\"Load image from URL and convert to Vertex AI Image format\"\"\"\n",
    "    response = requests.get(image_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    response.raise_for_status()\n",
    "    return Image.from_bytes(response.content)\n",
    "\n",
    "def analyze_relationship(model, image_url, so_title, so_body, llm_title, llm_body, prompt_type, model_name):\n",
    "    \"\"\"Analyze relationships between image, SO question, and LLM response\"\"\"\n",
    "    try:\n",
    "        image = load_image_from_url(image_url)\n",
    "        \n",
    "        instruction = f'''\n",
    "Analyze the following Stack Overflow post and provide ratings for three aspects:\n",
    "\n",
    "1. Image-Question Relationship: To what extent is the original Stack Overflow question related to the image posted? \n",
    "   Explain your reasoning and provide a rating between 1-10. In this question you look at the question title and body and see how much of it looks related to the image? Is the whole question revolving around the image or the image only talks about a part of the question. DO NOT focus on anything else besides the image and the Stack Overflow question.\n",
    "\n",
    "2. LLM-Image Relationship: To what extent does the {model_name} model's {prompt_type} generated question relate to the image?\n",
    "   Explain your reasoning and provide a rating between 1-10. In this question, answer how much of the response from LLM matched the image in a way that image might be related to a Stack Overflow question. DO NOT focus on anything else besides the image and the LLM generated response.\n",
    "\n",
    "3. LLM-SO Question Relationship: To what extent does the {model_name} model's {prompt_type} generated question relate to the original Stack Overflow question?\n",
    "   Explain your reasoning and provide a rating between 1-10. In this question, compare the LLM response with the original Stack Overflow question and give a similarity score kind of. DO NOT focus on anything else besides the Stack Overflow question and the LLM generated response.\n",
    "\n",
    "Original Stack Overflow:\n",
    "Title: {so_title}\n",
    "Body: {so_body}\n",
    "\n",
    "{model_name} {prompt_type} Generated:\n",
    "Title: {llm_title}\n",
    "Body: {llm_body}\n",
    "\n",
    "Please format your response exactly as follows:\n",
    "ANALYSIS:\n",
    "Image-Question Rating: [1-10]\n",
    "Reasoning: [Your explanation]\n",
    "\n",
    "LLM-Image Rating: [1-10]\n",
    "Reasoning: [Your explanation]\n",
    "\n",
    "LLM-SO Rating: [1-10]\n",
    "Reasoning: [Your explanation]\n",
    "'''\n",
    "\n",
    "        contents = [instruction, image]\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=0,\n",
    "            top_p=0.8,\n",
    "            top_k=40,\n",
    "            candidate_count=1,\n",
    "            max_output_tokens=2048,\n",
    "        )\n",
    "\n",
    "        response = model.generate_content(\n",
    "            contents,\n",
    "            generation_config=generation_config,\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_url}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_ratings(analysis_text):\n",
    "    \"\"\"Extract numerical ratings and reasoning from analysis text\"\"\"\n",
    "    try:\n",
    "        # Extract ratings using regex\n",
    "        image_question_rating = re.search(r'Image-Question Rating: (\\d+)', analysis_text)\n",
    "        llm_image_rating = re.search(r'LLM-Image Rating: (\\d+)', analysis_text)\n",
    "        llm_so_rating = re.search(r'LLM-SO Rating: (\\d+)', analysis_text)\n",
    "        \n",
    "        # Extract reasoning\n",
    "        image_question_reasoning = re.search(r'Image-Question Rating: \\d+\\nReasoning: (.*?)\\n\\nLLM', analysis_text, re.DOTALL)\n",
    "        llm_image_reasoning = re.search(r'LLM-Image Rating: \\d+\\nReasoning: (.*?)\\n\\nLLM', analysis_text, re.DOTALL)\n",
    "        llm_so_reasoning = re.search(r'LLM-SO Rating: \\d+\\nReasoning: (.*?)$', analysis_text, re.DOTALL)\n",
    "        \n",
    "        return {\n",
    "            'image_question_rating': int(image_question_rating.group(1)) if image_question_rating else None,\n",
    "            'llm_image_rating': int(llm_image_rating.group(1)) if llm_image_rating else None,\n",
    "            'llm_so_rating': int(llm_so_rating.group(1)) if llm_so_rating else None,\n",
    "            'image_question_reasoning': image_question_reasoning.group(1).strip() if image_question_reasoning else \"\",\n",
    "            'llm_image_reasoning': llm_image_reasoning.group(1).strip() if llm_image_reasoning else \"\",\n",
    "            'llm_so_reasoning': llm_so_reasoning.group(1).strip() if llm_so_reasoning else \"\",\n",
    "            'full_analysis': analysis_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting ratings: {str(e)}\")\n",
    "        return {\n",
    "            'image_question_rating': None,\n",
    "            'llm_image_rating': None,\n",
    "            'llm_so_rating': None,\n",
    "            'image_question_reasoning': \"\",\n",
    "            'llm_image_reasoning': \"\",\n",
    "            'llm_so_reasoning': \"\",\n",
    "            'full_analysis': analysis_text\n",
    "        }\n",
    "\n",
    "def analyze_single_id(post_id):\n",
    "    \"\"\"Analyze a single Stack Overflow post ID across all models and prompting strategies\"\"\"\n",
    "    # Initialize Vertex AI and model\n",
    "    text_model = init_vertexai()\n",
    "    \n",
    "    # Define models and prompting strategies\n",
    "    models = ['Gemini', 'GPT-4o', 'llama-3.2']\n",
    "    strategies = ['zero_shot', 'few_shot', 'cot']\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nAnalyzing Stack Overflow post ID: {post_id}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for model_name in models:\n",
    "        try:\n",
    "            # Load data for the specific model\n",
    "            file_path = f'Data/{model_name}/llm_responses_combined.csv'\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get the specific post\n",
    "            post = df[df['Id'] == post_id].iloc[0]\n",
    "            \n",
    "            # Extract image URL\n",
    "            image_urls = eval(post['ImageURLs'])  # Convert string representation of list to actual list\n",
    "            image_url = image_urls[0] if image_urls else None\n",
    "            \n",
    "            if not image_url:\n",
    "                print(f\"No image found for post ID {post_id} in {model_name} dataset\")\n",
    "                continue\n",
    "            \n",
    "            # Process each prompting strategy\n",
    "            for strategy in strategies:\n",
    "                print(f\"\\nProcessing {model_name} model with {strategy} prompting...\")\n",
    "                \n",
    "                # Get LLM responses for this strategy\n",
    "                llm_title = post[f'llm_{strategy}_title']\n",
    "                llm_body = post[f'llm_{strategy}_body']\n",
    "                \n",
    "                # Analyze relationships\n",
    "                analysis = analyze_relationship(\n",
    "                    text_model,\n",
    "                    image_url,\n",
    "                    post['Title'],\n",
    "                    post['Body'],\n",
    "                    llm_title,\n",
    "                    llm_body,\n",
    "                    strategy.replace('_', ' '),\n",
    "                    model_name\n",
    "                )\n",
    "                \n",
    "                # Extract ratings\n",
    "                ratings = extract_ratings(analysis)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'Id': post_id,\n",
    "                    'model': model_name,\n",
    "                    'strategy': strategy,\n",
    "                    **ratings\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Print analysis\n",
    "                print(f\"\\n{model_name} {strategy} Analysis:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(analysis)\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Add delay between analyses to avoid rate limiting\n",
    "                if not (model_name == models[-1] and strategy == strategies[-1]):\n",
    "                    print(\"\\nWaiting 10 seconds before next analysis...\")\n",
    "                    time.sleep(10)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name} for post ID {post_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save detailed results\n",
    "    output_file = f'Data/analysis_id_{post_id}_detailed.csv'\n",
    "    comparison_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Create and display summary tables\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Summary by model\n",
    "    print(\"\\nAverage Ratings by Model:\")\n",
    "    model_summary = comparison_df.groupby('model')[['image_question_rating', 'llm_image_rating', 'llm_so_rating']].mean()\n",
    "    print(model_summary)\n",
    "    \n",
    "    # Summary by strategy\n",
    "    print(\"\\nAverage Ratings by Prompting Strategy:\")\n",
    "    strategy_summary = comparison_df.groupby('strategy')[['image_question_rating', 'llm_image_rating', 'llm_so_rating']].mean()\n",
    "    print(strategy_summary)\n",
    "    \n",
    "    # Combined model-strategy summary\n",
    "    print(\"\\nDetailed Ratings by Model and Strategy:\")\n",
    "    detailed_summary = comparison_df.pivot_table(\n",
    "        index=['model', 'strategy'],\n",
    "        values=['image_question_rating', 'llm_image_rating', 'llm_so_rating'],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    print(detailed_summary)\n",
    "    \n",
    "    print(f\"\\nDetailed analysis saved to: {output_file}\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Interactive interface\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        try:\n",
    "            post_id = int(input(\"\\nEnter Stack Overflow post ID to analyze (or -1 to exit): \"))\n",
    "            if post_id == -1:\n",
    "                break\n",
    "            results = analyze_single_id(post_id)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid post ID\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        proceed = input(\"\\nWould you like to analyze another post? (y/n): \")\n",
    "        if proceed.lower() != 'y':\n",
    "            break\n",
    "            \n",
    "    print(\"\\nAnalysis complete. Thank you!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
